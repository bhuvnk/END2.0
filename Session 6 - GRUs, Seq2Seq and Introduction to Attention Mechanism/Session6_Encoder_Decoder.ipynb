{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session6_Encoder_Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvnk/END2.0/blob/main/Session%206%20-%20GRUs%2C%20Seq2Seq%20and%20Introduction%20to%20Attention%20Mechanism/Session6_Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojGQm2Xy3wut"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPhj6gnAnT2"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import spacy\n",
        "import pandas as pd \n",
        "import random\n",
        "import os, pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbT7e4su8vBN"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwn4oStE6PzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff16814e-d87b-476d-e0ca-8e58b40b4c98"
      },
      "source": [
        "# Import and print samples of the dataset\n",
        "path = 'tweets.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  labels\n",
            "0  Obama has called the GOP budget social Darwini...       1\n",
            "1  In his teen years, Obama has been known to use...       0\n",
            "2  IPA Congratulates President Barack Obama for L...       0\n",
            "3  RT @Professor_Why: #WhatsRomneyHiding - his co...       0\n",
            "4  RT @wardollarshome: Obama has approved more ta...       1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f065ae3d-317e-4aa6-ff16-26cd207c5555"
      },
      "source": [
        "print(\"Shape of the data: \", df.shape)\n",
        "print(\"Data value counts:\")\n",
        "print(df.labels.value_counts())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the data:  (1364, 2)\n",
            "Data value counts:\n",
            "0    931\n",
            "1    352\n",
            "2     81\n",
            "Name: labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBnFDdMLFS16"
      },
      "source": [
        "## Field Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "# Define text and target fields of the data\n",
        "Tweet = data.Field(sequential = True,\n",
        "                    tokenize = 'spacy',\n",
        "                    batch_first =True,\n",
        "                    include_lengths=True)\n",
        "\n",
        "Label = data.LabelField(tokenize ='spacy',\n",
        "                        is_target=True,\n",
        "                        batch_first =True,\n",
        "                        sequential =False)\n",
        "\n",
        "fields = [('tweet', Tweet),('label', Label)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMPOnCwFbWA"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56df7112-c34c-4953-a656-99b6a2f72375"
      },
      "source": [
        "# Creating dataset\n",
        "twitterDataset = data.TabularDataset(path=\"tweets.csv\",\n",
        "                                     format=\"CSV\",\n",
        "                                     fields=fields,\n",
        "                                     skip_header=True)\n",
        "\n",
        "# split the dataset into train and valid\n",
        "(train, valid) = twitterDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))\n",
        "\n",
        "print(\"Length of training data: \", len(train))\n",
        "print(\"Length of validation data: \", len(valid))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data:  1159\n",
            "Length of validation data:  205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69654d2a-4f8f-469e-edbe-900d4d3927d9"
      },
      "source": [
        "# print an example of the dataset\n",
        "print(vars(train.examples[10]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tweet': ['Even', 'CBS', 'wo', \"n't\", 'buy', 'bogus', 'WH', 'explanation', 'of', 'Obama', 'Supreme', 'Court', 'comments', '-', 'at', 'http://t.co/rkNdEmIy', '#', 'withnewt', '#', 'tpn', '#', 'tcot', '#', 'tlot', '#', 'tpp', '#', 'sgp'], 'label': '1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "MAX_VOCAB_SIZE = 5_000\n",
        "\n",
        "Tweet.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5653e51f-edc6-4fdf-d262-b4ad6c6b6835"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  4653\n",
            "Size of label vocab :  3\n",
            "Top 10 words appreared repeatedly : [('Obama', 1064), ('#', 798), (':', 780), ('.', 755), (',', 595), ('\"', 558), ('the', 534), ('RT', 513), ('to', 398), ('?', 392)]\n",
            "Labels :  defaultdict(None, {'0': 0, '1': 1, '2': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfFUrcfFd3w"
      },
      "source": [
        "## Create iterator objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                                                            batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.tweet),\n",
        "                                                            sort_within_batch=True,\n",
        "                                                            device = device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3vZhdaUFKrp"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yGJQAUFFiBO"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0qPCWoaF3cP"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):        \n",
        "        super().__init__()              \n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "    def init_h_c(self, batch_size):\n",
        "        zeros = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        return (zeros, zeros)\n",
        "                \n",
        "    def forward(self, text, text_lengths, debug=False):\n",
        "\n",
        "        # text = [batch size, text_length]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, text_length, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.encoder(embedded)\n",
        "        # print(f\"from Encoder; shape of hidden : {hidden.shape}, shape of cell {cell.shape}\")\n",
        "        # hidden = [num layers, batch size, hid dim]\n",
        "        # cell = [num layers, batch size, hid dim]\n",
        "\n",
        "        encoding = hidden.permute(1, 0, 2)\n",
        "        # encoding = [batch_size, num_layers, hid_dim]\n",
        "        \n",
        "        # Code to print the outputs at each step\n",
        "        if debug:          \n",
        "          assert output.shape[0] == 1, \\\n",
        "            \"Encoding at hidden states can only be printed for one tweet\"\n",
        "\n",
        "          print(\"\\n\")\n",
        "          fig = plt.figure(figsize=(20, 8))\n",
        "          for i in range(output.shape[1]):\n",
        "            ax = fig.add_subplot(1, 4, i+1)            \n",
        "            ax = sns.heatmap(output[0][i].detach().numpy().reshape(-1, 1), annot=True)\n",
        "            ax.set(title=f\"Hidden encodings State {i}\")\n",
        "          ax = fig.add_subplot(1, 4, i+2)\n",
        "          ax = sns.heatmap(encoding.detach().numpy().reshape(-1, 1), annot=True)\n",
        "          ax.set(title=\"Encoder Single Vector output\")\n",
        "          plt.show()  \n",
        "\n",
        "        return encoding#, (hidden, cell)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL-Dk2AddXyK"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez-r5_SOWjs1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):        \n",
        "        super().__init__()     \n",
        "\n",
        "        # LSTM layer\n",
        "        self.decoder = nn.LSTM(input_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden_states, debug=False):\n",
        "\n",
        "        hidden, cell = hidden_states\n",
        "\n",
        "        # print(input.shape)\n",
        "        output, (hidden, cell) = self.decoder(input)\n",
        "\n",
        "        # hidden = [num layers, batch size, hid dim]\n",
        "        # cell = [num layers, batch size, hid dim]\n",
        "        \n",
        "        # Final dense layer\n",
        "        dense_outputs = self.fc(hidden[0])\n",
        "        \n",
        "        # Code to print the outputs at each step\n",
        "        if debug:          \n",
        "          assert output.shape[0] == 1, \\\n",
        "            \"Decoding at hidden states can only be printed for one tweet\"\n",
        "          \n",
        "          print(\"\\n\")\n",
        "          fig = plt.figure(figsize=(10, 8))\n",
        "          for i in range(output.shape[1]):\n",
        "            ax = fig.add_subplot(1, 2, i+1)            \n",
        "            ax = sns.heatmap(output[0][i].detach().numpy().reshape(-1, 1), annot=True)\n",
        "            ax.set(title=f\"Hidden encodings State {i}\")\n",
        "          ax = fig.add_subplot(1, 2, i+2)\n",
        "          ax = sns.heatmap(hidden.detach().numpy().reshape(-1, 1), annot=True)\n",
        "          ax.set(title=\"Decoder output to FC Layer\")\n",
        "          plt.show() \n",
        "        return dense_outputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQxvaoxbxL4"
      },
      "source": [
        "## Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t1ASPOLb0cS"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, text, text_length, debug=False, print_outputs=False):\n",
        "    \n",
        "    encoding = self.encoder(text,\n",
        "                            text_length,\n",
        "                            debug=debug)\n",
        "\n",
        "    (hidden, cell)  = self.encoder.init_h_c(batch_size=len(text))\n",
        "\n",
        "    # print(f\"to Decoder; shape of hidden : {hidden.shape}, shape of cell {cell.shape}\")\n",
        "\n",
        "    output = self.decoder(encoding,\n",
        "                          (hidden, cell),\n",
        "                          debug=debug)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmJHmzlFftTL"
      },
      "source": [
        "## Create the model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eJCAkJ71ZpB"
      },
      "source": [
        "# N_LAYERS = 1\n",
        "\n",
        "# ENC_INPUT_DIM = len(Tweet.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 25\n",
        "# ENC_DROPOUT = 0.2\n",
        "\n",
        "# DEC_INPUT_DIM = ENC_HID_DIM\n",
        "# DEC_HID_DIM = 25\n",
        "# DEC_OUTPUT_DIM = len(Label.vocab)\n",
        "# DEC_DROPOUT = 0.2\n",
        "\n",
        "# enc = Encoder(ENC_INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "# dec = Decoder(DEC_INPUT_DIM, DEC_HID_DIM, DEC_OUTPUT_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "# model = Model(enc, dec)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3DoppiUdoDU",
        "outputId": "34152112-2b25-4a76-9be3-0c89379b0534"
      },
      "source": [
        "N_LAYERS = 1\n",
        "\n",
        "ENC_INPUT_DIM = len(Tweet.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 24\n",
        "ENC_DROPOUT = 0.3\n",
        "\n",
        "DEC_INPUT_DIM = ENC_HID_DIM\n",
        "DEC_HID_DIM = ENC_HID_DIM\n",
        "DEC_OUTPUT_DIM = len(Label.vocab)\n",
        "DEC_DROPOUT = ENC_DROPOUT\n",
        "\n",
        "enc = Encoder(ENC_INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(DEC_INPUT_DIM, DEC_HID_DIM, DEC_OUTPUT_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Model(enc, dec)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuLylDLdfEgu",
        "outputId": "8b4b025a-8676-44c4-a814-afc31aaf2e06"
      },
      "source": [
        "print(model)\n",
        "\n",
        "# No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(4653, 256)\n",
            "    (encoder): LSTM(256, 24, batch_first=True, dropout=0.3)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder): LSTM(24, 24, batch_first=True, dropout=0.3)\n",
            "    (fc): Linear(in_features=24, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "The model has 1,223,115 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVklO7cTfdst"
      },
      "source": [
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model on GPU\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20lBmVlAgaoV"
      },
      "source": [
        "# Train and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owGOwJHhfmhm"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7omhE1Zffmhp"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Retrieve text and no. of words\n",
        "        tweet, tweet_length = batch.tweet\n",
        "\n",
        "        predictions = model(tweet, tweet_length).squeeze(1)\n",
        "        # predictions = model(batch.tweet)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_DVGvkdfmhu"
      },
      "source": [
        "# Evaluate the model on validation set\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            # Retrieve text and no. of words\n",
        "            tweet, tweet_length = batch.tweet\n",
        "\n",
        "            predictions = model(tweet, tweet_length).squeeze(1)\n",
        "            # predictions = model(batch.tweet)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDl2bTMnfmhz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZuk3QZDf0E4"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q6jgIK2fmh1",
        "outputId": "6e01bb1e-4a86-4165-99f3-58656f770a57"
      },
      "source": [
        "N_EPOCHS = 15 #15\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Model training\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.152 | Train Acc: 5.66%\n",
            "\t Val. Loss: 1.121 |  Val. Acc: 6.25%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.124 | Train Acc: 5.66%\n",
            "\t Val. Loss: 1.080 |  Val. Acc: 48.01%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.096 | Train Acc: 27.06%\n",
            "\t Val. Loss: 1.030 |  Val. Acc: 65.18%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.060 | Train Acc: 46.15%\n",
            "\t Val. Loss: 0.971 |  Val. Acc: 70.54%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.015 | Train Acc: 59.75%\n",
            "\t Val. Loss: 0.911 |  Val. Acc: 75.00%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.968 | Train Acc: 65.83%\n",
            "\t Val. Loss: 0.846 |  Val. Acc: 75.45%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.918 | Train Acc: 68.36%\n",
            "\t Val. Loss: 0.793 |  Val. Acc: 75.45%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8ZPWfTfmh-"
      },
      "source": [
        "# Model testing\n",
        "model.load_state_dict(torch.load('saved-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7E6Ke58EID-"
      },
      "source": [
        "# Encoder-Decoder Architecture Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHXzzC76fmiB"
      },
      "source": [
        "# load weights and tokenizer\n",
        "\n",
        "path='./saved-model.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "# inference \n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                 \n",
        "    prediction = model(tensor, length_tensor, debug=True).squeeze(1)\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtsBI6GDfmiD"
      },
      "source": [
        "pred = classify_tweet(\"print the outputs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9HxwKZVpriT"
      },
      "source": [
        "print(\"Label predicted by the model: \", pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pki6Y2eIrj0c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}