{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Session5_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvnk/END2.0/blob/main/Session%205%20-%20First%20Hands-on%20Sentiment%20Classification/Session5_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz95i8lFf4IZ",
        "outputId": "6cc01ae3-f75e-43ff-c581-9b326b6ef093"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  3 21:49:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQRdpglr04oe",
        "outputId": "37812e43-24e4-432a-d7b0-9b11e78b9468"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp37-none-any.whl size=37070 sha256=2475174740f78aa1bfc3cf6b50e9efb357e806a156443b648c77802e68b1c782\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYPRxpHKFFPr"
      },
      "source": [
        "import pytreebank\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD6kRh91tqs7"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
        "# !unzip -q 'stanfordSentimentTreebank.zip'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br16DcFJHrGX"
      },
      "source": [
        "out_path = os.path.join(sys.path[0], 'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('/content/stanfordSentimentTreebank')\n",
        "\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] + 1,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7-C26jWHDmJ"
      },
      "source": [
        "train_df1 = pd.read_csv('./sst_train.txt',sep='\\t',header=None,names=['label','text']) # reading the training set data\n",
        "\n",
        "valid_df = pd.read_csv('./sst_dev.txt',sep='\\t',header=None,names=['label','text']) # reading the validation set data\n",
        "# train_df1 = pd.concat([train_df1, dev_df], ignore_index=True) # Concatinating dev with train\n",
        "\n",
        "test_df = pd.read_csv('./sst_test.txt',sep='\\t',header=None,names=['label','text']) # reading the test set data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "Sb2fnle0Pcg_",
        "outputId": "530aa09b-efe2-4419-9f8c-72aad5edb2ef"
      },
      "source": [
        "train_df1.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      4  The Rock is destined to be the 21st Century 's...\n",
              "1      5  The gorgeously elaborate continuation of `` Th...\n",
              "2      4  Singer/composer Bryan Adams contributes a slew...\n",
              "3      3  You 'd think by now America would have had eno...\n",
              "4      4               Yet the act is still charming here ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqlSQlluvzcU",
        "outputId": "e1ce2790-e1e5-45a1-b6d5-846014a169aa"
      },
      "source": [
        "train_df1.shape, valid_df.shape, test_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8544, 2), (1101, 2), (2210, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0680KTJjOcFT"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTA50QFC8_sy"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Seems to be one of the worse Augmentation when it comes to NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWC2tLVOJ-oJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6d9e11-4ca2-4e71-8a66-829505a3d19f"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.6MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=4599b0cb6f94c3cc8bdeaeb26f9cf834f9b6d94ee03e9ab346a50edc3b12c5e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hstspreload, rfc3986, hyperframe, hpack, h2, h11, sniffio, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbBv8NeMP4Ch"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        " \n",
        "translator = googletrans.Translator()\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH87Fx1NQMHC",
        "outputId": "2fed6051-c0c5-4d71-c056-c5bfe3762a5b"
      },
      "source": [
        "vailable_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to sundanese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbW-TnVk6q-b"
      },
      "source": [
        "# Trying the backtranslation on random 200 sentences\n",
        "sample = train_df1.sample(2000)\n",
        "text = list(sample.text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6_HPawWAk1"
      },
      "source": [
        "#translating to.\n",
        "translations = translator.translate(text, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "# print(t_text)\n",
        "\n",
        "#and back\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "# print(en_text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akb-MG6kYWtJ",
        "outputId": "4575958e-26d2-4e23-ee8f-7f45cd72af3b"
      },
      "source": [
        "back_trans_df=pd.DataFrame({'text':en_text,'label':list(sample.label)})\n",
        "back_trans_df.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "k8fS2aBGVkCU",
        "outputId": "e9847d4b-4eee-4164-ad4c-e4153dc5dd67"
      },
      "source": [
        "back_trans_df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ieu ngan ukur nyerat teu puguh.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Éta pilem pilem baheula anu hadé, anu hartosna...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bogdanovich ngetok jero kana mistik Hearst, ng...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anjeun tiasa ningali dimana Bad Bad Love nyobi...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>... pasemon da'wah ditata ku sentuhan ballet p...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0                    Ieu ngan ukur nyerat teu puguh.      1\n",
              "1  Éta pilem pilem baheula anu hadé, anu hartosna...      4\n",
              "2  Bogdanovich ngetok jero kana mistik Hearst, ng...      5\n",
              "3  Anjeun tiasa ningali dimana Bad Bad Love nyobi...      3\n",
              "4  ... pasemon da'wah ditata ku sentuhan ballet p...      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZmLCklbc47p"
      },
      "source": [
        "# train_df_agumented = pd.concat([train_df,d1])\n",
        "# train_df_agumented = train_df_agumented.reset_index(drop=True)\n",
        "# train_df_agumented.shape"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ7mehfu9NW4"
      },
      "source": [
        "### Radom swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19pwfeviZI18"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEzYYp3x9YWh"
      },
      "source": [
        "data_rand_swap=pd.DataFrame()\n",
        "data_rand_swap['text']=train_df1['text'].apply(lambda x : \" \".join(random_swap(x.split(' '))))\n",
        "data_rand_swap['label'] = train_df1['label'] "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLvrdVP7Rren",
        "outputId": "da882293-3ec5-47d7-8858-a42ca7e27e18"
      },
      "source": [
        "data_rand_swap.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiOxMPMhBPJi"
      },
      "source": [
        "# train_df_agumented1=pd.concat([train_df_agumented,data_rand_swap])\n",
        "# train_df_agumented1=train_df_agumented1.reset_index(drop=True)\n",
        "# train_df_agumented1.shape"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bc4cU_ABnDU"
      },
      "source": [
        "### Random Deletion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kw1zkbYeLAD"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-IyaIrrZoh2"
      },
      "source": [
        "data_rand_del=pd.DataFrame()\n",
        "data_rand_del['text'] = train_df1['text'].apply(lambda x : \" \".join(random_deletion(x.split(' '))))\n",
        "data_rand_del['label'] = train_df1['label'] "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-z9pVvTR6OG",
        "outputId": "8433d0f6-c9d4-4d9a-ae8e-947b294cab00"
      },
      "source": [
        "data_rand_del.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUu5fSTCLKK"
      },
      "source": [
        "# train_df_agumented2=pd.concat([train_df_agumented1,data_rand_del])\n",
        "# train_df_agumented2=train_df_agumented2.reset_index(drop=True)\n",
        "# train_df_agumented2.shape"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fZAKLbUj6E"
      },
      "source": [
        "## combining augmented data with original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "tpTaEMCsUoXc",
        "outputId": "998e9697-f087-46a4-ecb3-7f0ab55eb546"
      },
      "source": [
        "train_df2 = pd.concat([train_df1, back_trans_df, data_rand_swap, data_rand_del], ignore_index=True)\n",
        "train_df2.sample(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11719</th>\n",
              "      <td>2</td>\n",
              "      <td>I version of think most n't people the who do ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9124</th>\n",
              "      <td>2</td>\n",
              "      <td>You get the impression that writer and directo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3094</th>\n",
              "      <td>5</td>\n",
              "      <td>Not only is it a charming , funny and beautifu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>1</td>\n",
              "      <td>A loud , low-budget and tired formula film tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19205</th>\n",
              "      <td>5</td>\n",
              "      <td>Jeffrey Tambor performance intelligent jazz-pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>2</td>\n",
              "      <td>But the movie . muddled hook is effectively wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9520</th>\n",
              "      <td>4</td>\n",
              "      <td>I 'm not a fan of the phrase ` life affirming ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12707</th>\n",
              "      <td>5</td>\n",
              "      <td>Spielberg 's deserved real masterpiece later s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3396</th>\n",
              "      <td>4</td>\n",
              "      <td>Precocious smarter-than-thou wayward teen stru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9583</th>\n",
              "      <td>3</td>\n",
              "      <td>Especially compared with the television series...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                               text\n",
              "11719      2  I version of think most n't people the who do ...\n",
              "9124       2  You get the impression that writer and directo...\n",
              "3094       5  Not only is it a charming , funny and beautifu...\n",
              "5189       1  A loud , low-budget and tired formula film tha...\n",
              "19205      5  Jeffrey Tambor performance intelligent jazz-pl...\n",
              "15999      2  But the movie . muddled hook is effectively wa...\n",
              "9520       4  I 'm not a fan of the phrase ` life affirming ...\n",
              "12707      5  Spielberg 's deserved real masterpiece later s...\n",
              "3396       4  Precocious smarter-than-thou wayward teen stru...\n",
              "9583       3  Especially compared with the television series..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "tK3qWu2nW8uw",
        "outputId": "386624b6-db38-4474-8deb-5601eb049081"
      },
      "source": [
        "train_df2.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0b9ce0dae4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "K0VMofPH_Llt",
        "outputId": "20989f47-44df-47b2-b275-7122f64826f6"
      },
      "source": [
        "tran_df = train_df2.sample(train_df2.shape//2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8b163d69146f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtran_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pvhq6M3Y-pR"
      },
      "source": [
        "# Data Preparation for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oajs2jeOYrtK"
      },
      "source": [
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import legacy\n",
        "from torchtext.legacy import data\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zishy5CxiKR"
      },
      "source": [
        "Text = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz_kRHP1y2RW"
      },
      "source": [
        "#Mapping dataframe fields to tensor fields \n",
        "fields=[('text',Text),('label',Label)]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odpi57Wvy7Qn"
      },
      "source": [
        "Converting each sample into pyrorch example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UFNohxTy-sj"
      },
      "source": [
        "example_train= [data.Example.fromlist([train_df.text[i],train_df.label[i]], fields) for i in range(train_df.shape[0])]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tdBOnate-41"
      },
      "source": [
        "# Creating dataset\n",
        "train = data.Dataset(example_train, fields)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaCF8vpSMgn"
      },
      "source": [
        "example_valid=[data.Example.fromlist([valid_df.text[i],valid_df.label[i]],fields) for i in range (valid_df.shape[0])]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CMpVP36fCGT"
      },
      "source": [
        "valid = data.Dataset(example_valid, fields)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOXcLcCeY-RF"
      },
      "source": [
        "example_test=[data.Example.fromlist([test_df.text[i],test_df.label[i]],fields) for i in range (test_df.shape[0])]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B2Gybja8J0q"
      },
      "source": [
        "test = data.Dataset(example_test, fields)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQRD-yBo2rd6",
        "outputId": "aeee9f8e-ad0f-4707-ade9-9f2c0f5dc730"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 5,\n",
              " 'text': ['Good',\n",
              "  'fun',\n",
              "  ',',\n",
              "  'good',\n",
              "  'action',\n",
              "  ',',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ',',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ',',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ',',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9b0eXCh2s5U",
        "outputId": "acbda083-b54e-4049-eb74-d6874e7edce9"
      },
      "source": [
        "vars(valid.examples[10])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 5,\n",
              " 'text': ['Unlike',\n",
              "  'the',\n",
              "  'speedy',\n",
              "  'wham',\n",
              "  '-',\n",
              "  'bam',\n",
              "  'effect',\n",
              "  'of',\n",
              "  'most',\n",
              "  'Hollywood',\n",
              "  'offerings',\n",
              "  ',',\n",
              "  'character',\n",
              "  'development',\n",
              "  '--',\n",
              "  'and',\n",
              "  'more',\n",
              "  'importantly',\n",
              "  ',',\n",
              "  'character',\n",
              "  'empathy',\n",
              "  '--',\n",
              "  'is',\n",
              "  'at',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'Italian',\n",
              "  'for',\n",
              "  'Beginners',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVtYPo3F29Ib"
      },
      "source": [
        "Building the Vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-V-7jMA249T"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Text.build_vocab(train,\n",
        "                 max_size = MAX_VOCAB_SIZE\n",
        "                 ) #, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CSn6DxHce9G",
        "outputId": "b1ecf547-8244-4853-dbd8-6b5cdf656ba2"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(Text.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(Label.vocab)}\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 18686\n",
            "Unique tokens in LABEL vocabulary: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTarLwAi3XSU"
      },
      "source": [
        "Wanted some 25_000 vocab size but looks like we don't have enough words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az74bSxG3agX",
        "outputId": "e280d168-313f-42f3-db3e-8a51680275cf"
      },
      "source": [
        "print('Size of input vocab : ', len(Text.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Text.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  18686\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 22012), (',', 19449), ('the', 16326), ('and', 12120), ('a', 11964), ('of', 11929), ('to', 8184), ('-', 7475), (\"'s\", 6839), ('is', 6802)]\n",
            "Labels :  defaultdict(None, {4: 0, 2: 1, 3: 2, 5: 3, 1: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylVwZC8X3ejr"
      },
      "source": [
        "A lot of stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W8RhfVRNNYy",
        "outputId": "73848644-8483-4dcf-9cf5-0558d9677bb6"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "netM3ja7NfO5"
      },
      "source": [
        "train_iterator = data.BucketIterator(train, \n",
        "                                     batch_size = BATCH_SIZE,\n",
        "                                     sort_key = lambda x: len(x.text),\n",
        "                                     sort_within_batch=True, \n",
        "                                     device = device)\n",
        "valid_iterator = data.BucketIterator(valid,\n",
        "                                     batch_size = BATCH_SIZE, \n",
        "                                     sort_key = lambda x: len(x.text),\n",
        "                                     sort_within_batch=True,\n",
        "                                     device = device)\n",
        "test_iterator = data.BucketIterator(test,\n",
        "                                     batch_size = BATCH_SIZE, \n",
        "                                     sort_key = lambda x: len(x.text),\n",
        "                                     sort_within_batch=True,\n",
        "                                     device = device)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zal53-iI_cn",
        "outputId": "a8235c7c-e098-48be-c531-85ef349e728e"
      },
      "source": [
        "print('Train data dims')\n",
        "for batch in train_iterator:\n",
        "    print(f'input_text: {batch.text[0].size()}, length: {batch.text[1].size()}')\n",
        "    print(f'Target: {batch.label.size()}')\n",
        "    break"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data dims\n",
            "input_text: torch.Size([128, 26]), length: torch.Size([128])\n",
            "Target: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlxZdebk1q-M"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Text.vocab.stoi, tokens)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH_TqIWiOkWO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                          #  bidirectional=True,\n",
        "                           )\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWo1uK-hQY1K"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Text.vocab)\n",
        "embedding_dim = 256\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "PAD_IDX = Text.vocab.stoi[Text.pad_token]\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZGxpmgvQemB",
        "outputId": "c129fe7e-0d2f-4eb8-f5ef-80830b509572"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(18686, 256)\n",
            "  (encoder): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,837,573 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW9KtmHiQjuO"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fR455AoQ5sV"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        #tweet, tweet_lengths = batch.tweet  \n",
        "        text,text_length = batch.text\n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        #predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        predictions=model(text,text_length)\n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrSM-4pdQ65t"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            #tweet, tweet_lengths = batch.tweet\n",
        "            text,text_length = batch.text\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(text, text_length).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3yKaXpIQ9xv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')\n",
        "import numpy as np"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVvM6z6gRChv",
        "outputId": "8c35ab72-8704-427e-dfc7-b06f37d6cf57"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "epoch_list = np.zeros((0))\n",
        "train_loss_list = np.zeros((0))\n",
        "valid_loss_list = np.zeros((0))\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights_lstm.pt')\n",
        "    \n",
        "    print(f'\\t epoch : {epoch} |\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "    epoch_list = np.append(epoch_list, [epoch])\n",
        "    train_loss_list= np.append(train_loss_list, [train_loss])\n",
        "    valid_loss_list= np.append(valid_loss_list, [valid_loss])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t epoch : 0 |\tTrain Loss: 1.576 | Train Acc: 28.47%\n",
            "\t Val. Loss: 1.583 |  Val. Acc: 27.60% \n",
            "\n",
            "\t epoch : 1 |\tTrain Loss: 1.540 | Train Acc: 34.77%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 33.55% \n",
            "\n",
            "\t epoch : 2 |\tTrain Loss: 1.497 | Train Acc: 39.84%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.31% \n",
            "\n",
            "\t epoch : 3 |\tTrain Loss: 1.454 | Train Acc: 44.51%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 34.68% \n",
            "\n",
            "\t epoch : 4 |\tTrain Loss: 1.419 | Train Acc: 48.40%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 34.45% \n",
            "\n",
            "\t epoch : 5 |\tTrain Loss: 1.385 | Train Acc: 51.98%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.86% \n",
            "\n",
            "\t epoch : 6 |\tTrain Loss: 1.353 | Train Acc: 55.47%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 34.42% \n",
            "\n",
            "\t epoch : 7 |\tTrain Loss: 1.324 | Train Acc: 58.42%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 35.03% \n",
            "\n",
            "\t epoch : 8 |\tTrain Loss: 1.304 | Train Acc: 60.48%\n",
            "\t Val. Loss: 1.556 |  Val. Acc: 31.96% \n",
            "\n",
            "\t epoch : 9 |\tTrain Loss: 1.267 | Train Acc: 64.55%\n",
            "\t Val. Loss: 1.567 |  Val. Acc: 31.39% \n",
            "\n",
            "\t epoch : 10 |\tTrain Loss: 1.241 | Train Acc: 67.53%\n",
            "\t Val. Loss: 1.554 |  Val. Acc: 33.64% \n",
            "\n",
            "\t epoch : 11 |\tTrain Loss: 1.210 | Train Acc: 70.82%\n",
            "\t Val. Loss: 1.571 |  Val. Acc: 31.33% \n",
            "\n",
            "\t epoch : 12 |\tTrain Loss: 1.189 | Train Acc: 73.29%\n",
            "\t Val. Loss: 1.573 |  Val. Acc: 31.18% \n",
            "\n",
            "\t epoch : 13 |\tTrain Loss: 1.164 | Train Acc: 75.68%\n",
            "\t Val. Loss: 1.572 |  Val. Acc: 31.56% \n",
            "\n",
            "\t epoch : 14 |\tTrain Loss: 1.138 | Train Acc: 78.22%\n",
            "\t Val. Loss: 1.561 |  Val. Acc: 32.48% \n",
            "\n",
            "\t epoch : 15 |\tTrain Loss: 1.123 | Train Acc: 79.58%\n",
            "\t Val. Loss: 1.586 |  Val. Acc: 29.30% \n",
            "\n",
            "\t epoch : 16 |\tTrain Loss: 1.104 | Train Acc: 81.45%\n",
            "\t Val. Loss: 1.573 |  Val. Acc: 31.44% \n",
            "\n",
            "\t epoch : 17 |\tTrain Loss: 1.091 | Train Acc: 82.67%\n",
            "\t Val. Loss: 1.577 |  Val. Acc: 31.30% \n",
            "\n",
            "\t epoch : 18 |\tTrain Loss: 1.075 | Train Acc: 84.07%\n",
            "\t Val. Loss: 1.584 |  Val. Acc: 30.72% \n",
            "\n",
            "\t epoch : 19 |\tTrain Loss: 1.065 | Train Acc: 85.06%\n",
            "\t Val. Loss: 1.593 |  Val. Acc: 29.22% \n",
            "\n",
            "\t epoch : 20 |\tTrain Loss: 1.057 | Train Acc: 85.75%\n",
            "\t Val. Loss: 1.600 |  Val. Acc: 28.46% \n",
            "\n",
            "\t epoch : 21 |\tTrain Loss: 1.050 | Train Acc: 86.44%\n",
            "\t Val. Loss: 1.591 |  Val. Acc: 29.68% \n",
            "\n",
            "\t epoch : 22 |\tTrain Loss: 1.044 | Train Acc: 86.93%\n",
            "\t Val. Loss: 1.579 |  Val. Acc: 31.47% \n",
            "\n",
            "\t epoch : 23 |\tTrain Loss: 1.034 | Train Acc: 87.91%\n",
            "\t Val. Loss: 1.581 |  Val. Acc: 30.84% \n",
            "\n",
            "\t epoch : 24 |\tTrain Loss: 1.027 | Train Acc: 88.48%\n",
            "\t Val. Loss: 1.584 |  Val. Acc: 30.52% \n",
            "\n",
            "\t epoch : 25 |\tTrain Loss: 1.026 | Train Acc: 88.58%\n",
            "\t Val. Loss: 1.563 |  Val. Acc: 33.04% \n",
            "\n",
            "\t epoch : 26 |\tTrain Loss: 1.022 | Train Acc: 89.00%\n",
            "\t Val. Loss: 1.566 |  Val. Acc: 33.04% \n",
            "\n",
            "\t epoch : 27 |\tTrain Loss: 1.015 | Train Acc: 89.59%\n",
            "\t Val. Loss: 1.591 |  Val. Acc: 30.11% \n",
            "\n",
            "\t epoch : 28 |\tTrain Loss: 1.013 | Train Acc: 89.64%\n",
            "\t Val. Loss: 1.592 |  Val. Acc: 29.27% \n",
            "\n",
            "\t epoch : 29 |\tTrain Loss: 1.010 | Train Acc: 89.94%\n",
            "\t Val. Loss: 1.594 |  Val. Acc: 29.56% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_G54tcsgQ4A"
      },
      "source": [
        "## on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT6SuEr3gTEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fda8eb3-6a95-4016-c8d4-bbffacf0e1d0"
      },
      "source": [
        "model.load_state_dict(torch.load('saved_weights_lstm.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.546 | Test Acc: 33.14%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5k3SxkqCiF6"
      },
      "source": [
        "## Few Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nq37g6OGlsN"
      },
      "source": [
        "#inference \n",
        "path='./saved_weights_lstm.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_sentences(sent):\n",
        "        \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sent)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    #return categories[pred.item()]\n",
        "    return pred.item()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx48TZsIGMko",
        "outputId": "dead1b4f-b096-4f59-d15a-f5c9becb9067"
      },
      "source": [
        "for i in range(10):\n",
        "  sent = ' '.join(test.examples[i].text)\n",
        "  pred = classify_sentences(sent)\n",
        "  print(f\"Input: {sent}, \\n \\t label : {test.examples[i].label} \\t predicted : {pred} \\n\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Effective but too - tepid biopic, \n",
            " \t label : 3 \t predicted : 1 \n",
            "\n",
            "Input: If you sometimes like to go to the movies to have fun , Wasabi is a good place to start ., \n",
            " \t label : 4 \t predicted : 2 \n",
            "\n",
            "Input: Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one ., \n",
            " \t label : 5 \t predicted : 1 \n",
            "\n",
            "Input: The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game ., \n",
            " \t label : 3 \t predicted : 0 \n",
            "\n",
            "Input: Offers that rare combination of entertainment and education ., \n",
            " \t label : 5 \t predicted : 0 \n",
            "\n",
            "Input: Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions ., \n",
            " \t label : 4 \t predicted : 1 \n",
            "\n",
            "Input: Steers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it ., \n",
            " \t label : 4 \t predicted : 0 \n",
            "\n",
            "Input: But he somehow pulls it off ., \n",
            " \t label : 4 \t predicted : 1 \n",
            "\n",
            "Input: Take Care of My Cat offers a refreshingly different slice of Asian cinema ., \n",
            " \t label : 4 \t predicted : 0 \n",
            "\n",
            "Input: This is a film well worth seeing , talking and singing heads and all ., \n",
            " \t label : 5 \t predicted : 1 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8cSd0krHukl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}